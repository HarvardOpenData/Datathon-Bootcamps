{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium Bootcamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Selenium?\n",
    "\n",
    "Most of the time, scraping methods like RegEx or BeautifulSoup will be fine for dealing with websites. However, some websites handle things a little bit differently. Let's take a look. Run the following two cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"open my_website.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try downloading the site and scraping the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "my_website_url = \"file://\" + os.getcwd() + \"/my_website.html\"\n",
    "html = str(urlopen(my_website_url).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seems to work fine. Let's try scraping some data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data = re.findall(r'<td class = \"static_input\">(.+?)<\\/td><td class = \"static_output\">(.+?)<\\/td>', html)\n",
    "static_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks good! But I think we have some data missing... Not a problem, let's try to scrape it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_data = re.findall(r'<td class = \"dynamic_input\">(.+?)<\\/td><td class = \"dynamic_output\">(.+?)<\\/td>', html)\n",
    "dynamic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What went wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cases where you might need Selenium\n",
    "* Data is generated via interaction e.g. searching, clicking more, etc.\n",
    "* Data is generated via \"ajax\" requests\n",
    "* Website requires login of some kind\n",
    "* Dealing with the html parsing and regex is just too damn annoying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Instructions\n",
    "\n",
    "1. Install Selenium for Python. ```python3 -m pip install selenium```. [Full Instructions](https://selenium-python.readthedocs.io/installation.html)\n",
    "\n",
    "2. [Install chrome webdriver](https://sites.google.com/a/chromium.org/chromedriver/downloads).\n",
    "\n",
    "3. Move the resulting file to this folder.\n",
    "\n",
    "\n",
    "### Great! Now let's get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver\")\n",
    "\n",
    "# for Windows users\n",
    "# driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helper functions that will be useful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_until_present(driver, time, *locator):\n",
    "    return WebDriverWait(driver, time).until(expected_conditions.presence_of_element_located(locator))\n",
    "\n",
    "def find_element_by_text(driver, text):\n",
    "    return driver.find_element_by_xpath(\"//*[contains(text(), '{}')]\".format(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try navigating to a url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"http://google.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not too bad, now let's see if we can interact with the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_box = wait_until_present(driver, 10, By.NAME, \"q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_box.send_keys(\"Harvard\" + Keys.ENTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick demo of what this can be useful for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_divs = driver.find_elements_by_class_name(\"r\")\n",
    "link_elts = [div.find_element_by_tag_name(\"a\") for div in result_divs if div.text != \"\"]\n",
    "links = [(elt.text, elt.get_attribute(\"href\")) for elt in link_elts]\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try to do some basic interaction! Try navigating to my website from earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION ###\n",
    "driver.get(my_website_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It'd be nice if we could click the button... Let's try to get that button into a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION ###\n",
    "my_button = driver.find_element_by_id(\"btnMore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's click it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The new data is here! Let's try to scrape it. First, let's find the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION ###\n",
    "dynamic_table = driver.find_element_by_id(\"dynamic_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's get the row of each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = dynamic_table.find_elements_by_tag_name(\"tr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great, now let's get the data within each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for row in rows[1:]:\n",
    "    ### SOLUTION ###\n",
    "    input_elt = row.find_elements_by_class_name(\"dynamic_input\")[0]\n",
    "    output_elt = row.find_elements_by_class_name(\"dynamic_output\")[0]\n",
    "    data.append((input_elt.text, output_elt.text))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See? Not so bad! But that was kind of long... what if we could combine regex with Selenium?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we can use RegEx!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION ###\n",
    "data = re.findall(r'<td class=\"dynamic_input\">(\\d+?)<\\/td><td class=\"dynamic_output\">(\\d+?)<\\/td>', html)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now for one of the best parts: the ability to navigate. Let's try scraping Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"http://canvas.harvard.edu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see if we can get the todo titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION ###\n",
    "todo_elts = driver.find_elements_by_class_name(\"todo-details__title\")\n",
    "todo_titles = [elt.text for elt in todo_elts]\n",
    "todo_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hmm... doesn't tell us that much. Let's get which classes they are for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todo_class_elts = driver.find_elements_by_class_name(\"todo-details__context\")\n",
    "todo_classes = [elt.text for elt in todo_class_elts]\n",
    "todo_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todos = list(zip(todo_classes, todo_titles))\n",
    "todos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'll leave it to you as a challenge to see if you can get due dates :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and cons of Selenium\n",
    "### Pros\n",
    "* Very powerful, can get a lot done\n",
    "* Pretty intuitive, like using a browser\n",
    "* Can download the current HTML representation, not just the intitial one\n",
    "\n",
    "### Cons\n",
    "* Pretty heavyweight, need to install a lot\n",
    "* Can get annoying if you need to be fully automated\n",
    "* Can be substantially slower than RegEx or BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on ethical scraping\n",
    "Scraping methods, especially like Selenium can give you a *lot* of power. Make sure you use it responsibly. Don't violate individual privacy, and make sure you check the user agreements of websites before you scrape them. Recently a court case ruled that it was legal to scrape LinkedIn, but even then, be careful. A lot of the information that you scrape is still subject to copyright law and who knows what might happen legally in the future with this kind of thing. But in general, be responsible with scraping. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
